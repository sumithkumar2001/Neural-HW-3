# Deep Learning Projects with TensorFlow & Keras

This repository contains four deep learning projects implemented using TensorFlow and Keras. Each project explores a different neural network architecture and application, from image reconstruction to sentiment analysis.

---

## ğŸ“Œ 1. Basic Autoencoder for Image Reconstruction

### âœ… Overview
An autoencoder is a neural network trained to reconstruct its input. In this project, a fully connected autoencoder is trained on the MNIST dataset to compress and decompress images of handwritten digits.

### ğŸ§  Architecture
- **Encoder**: Input (784) â†’ Dense(32)
- **Decoder**: Dense(32) â†’ Output (784)

### ğŸ› ï¸ Highlights
- Uses `binary_crossentropy` loss for pixel-wise reconstruction.
- Visualizes original vs reconstructed digits.
- Tests latent space sizes (16, 32, 64) to evaluate reconstruction quality.

### ğŸ” Key Insight
Larger latent dimensions preserve more detail, while smaller ones force the model to generalize and compress better.

---

## ğŸ§¼ 2. Denoising Autoencoder

### âœ… Overview
A denoising autoencoder is trained to reconstruct clean images from noisy inputs. Gaussian noise is added to MNIST digits, and the model learns to remove it.

### ğŸ§  Architecture
Same as the basic autoencoder.

### ğŸ› ï¸ Highlights
- Adds Gaussian noise (`np.random.normal`) with `std=0.5`.
- Trains on noisy input but predicts clean output.
- Visualizes Noisy â†’ Denoised â†’ Original images.
- Compares with a regular autoencoder to show noise resilience.

### ğŸŒ Real-World Use Case
Used in **medical imaging** to enhance scans by removing noise caused by low resolution or equipment limitations.

---

## âœï¸ 3. Text Generation with LSTM (Character-Level RNN)

### âœ… Overview
An LSTM-based Recurrent Neural Network is trained to predict the next character in a text sequence using "The Little Prince" as training data.

### ğŸ§  Architecture
- Input: One-hot encoded characters
- LSTM(128) â†’ Dense(vocab_size, softmax)

### ğŸ› ï¸ Highlights
- Character-level model captures grammar and syntax.
- Generates text by sampling characters based on predicted probabilities.
- Demonstrates **temperature scaling** to control creativity/randomness.

### ğŸ”¥ Temperature Scaling Explained
- Low (0.2): Conservative, repetitive
- Medium (1.0): Balanced
- High (1.5): Creative, but prone to errors

---

## ğŸ˜Š 4. Sentiment Classification using LSTM

### âœ… Overview
An LSTM-based binary classifier trained on the IMDB movie review dataset to detect positive or negative sentiment.

### ğŸ§  Architecture
- Embedding â†’ LSTM(128) â†’ Dense(1, sigmoid)

### ğŸ› ï¸ Highlights
- Uses Keras `imdb` dataset (tokenized, padded).
- Evaluates model with:
  - Accuracy
  - Precision
  - Recall
  - F1-score
  - Confusion Matrix

### ğŸ”„ Precision vs Recall in Sentiment Tasks
- **High Recall**: Catch all negative reviews (even with some false positives).
- **High Precision**: Ensure flagged reviews are truly negative.

Ideal tradeoff depends on business use (e.g., customer service, moderation).

---

## ğŸ”§ Requirements

- Python 3.7+
- TensorFlow 2.x
- NumPy, Matplotlib, Seaborn
- scikit-learn (for metrics)

Install via:

```bash
pip install tensorflow numpy matplotlib seaborn scikit-learn
